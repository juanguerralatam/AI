{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "from langchain_core.tools import tool\n",
    "from langchain_deepseek import ChatDeepSeek\n",
    "\n",
    "def _set_if_undefined(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"Please provide your {var}\")\n",
    "\n",
    "_set_if_undefined(\"DEEPSEEK_API_KEY\")\n",
    "\n",
    "llm = ChatDeepSeek(\n",
    "    model=\"deepseek-chat\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=10, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'multiply',\n",
       "  'args': {'a': 2, 'b': 3},\n",
       "  'id': 'call_0_b39ac643-848e-4663-a8fe-3cc8747048fc',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Schema for structured output\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class SearchQuery(BaseModel):\n",
    "    search_query: str = Field(None, description=\"Query that is optimized web search.\")\n",
    "    justification: str = Field(\n",
    "        None, description=\"Why this query is relevant to the user's request.\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Augment the LLM with schema for structured output\n",
    "structured_llm = llm.with_structured_output(SearchQuery)\n",
    "\n",
    "# Invoke the augmented LLM\n",
    "output = structured_llm.invoke(\"How does Calcium CT score relate to high cholesterol?\")\n",
    "\n",
    "# Define a tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    return a * b\n",
    "\n",
    "# Augment the LLM with tools\n",
    "llm_with_tools = llm.bind_tools([multiply])\n",
    "\n",
    "# Invoke the LLM with input that triggers the tool call\n",
    "msg = llm_with_tools.invoke(\"What is 2 times 3?\")\n",
    "\n",
    "# Get the tool call\n",
    "msg.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial joke:\n",
      "Sure! Here's a quick one:  \n",
      "\n",
      "**Why don‚Äôt cats play poker in the wild?**  \n",
      "*Too many cheetahs!*  \n",
      "\n",
      "(Or if you prefer a classic: **\"I told my cat I was reading a book about anti-gravity. He couldn‚Äôt put it down.\"**)  \n",
      "\n",
      "Hope that gives you a chuckle! üò∏\n",
      "\n",
      "--- --- ---\n",
      "\n",
      "Improved joke:\n",
      "Here‚Äôs a funnier version with extra wordplay:  \n",
      "\n",
      "**Why don‚Äôt cats play poker in the wild?**  \n",
      "*Too many cheetahs‚Äîthey always *spotted* the bluff!*  \n",
      "\n",
      "And for the classic:  \n",
      "**\"I told my cat I was reading a book about anti-gravity. He couldn‚Äôt put it down‚Äîguess he‚Äôs *paws-itively* hooked!\"**  \n",
      "\n",
      "Now it‚Äôs a *purr-fect* combo of puns! üò∏üé≤\n",
      "\n",
      "--- --- ---\n",
      "\n",
      "Final joke:\n",
      "Here‚Äôs a *wildly* unexpected twist to your already hilarious jokes‚Äînow with an extra layer of absurdity and wordplay:  \n",
      "\n",
      "---  \n",
      "\n",
      "**Why don‚Äôt cats play poker in the wild?**  \n",
      "*Too many cheetahs‚Äîthey always *spotted* the bluff!*  \n",
      "**But the real reason?** *The ante-lopes kept raising the stakes.* ü¶åüé∞  \n",
      "\n",
      "---  \n",
      "\n",
      "**\"I told my cat I was reading a book about anti-gravity. He couldn‚Äôt put it down‚Äîguess he‚Äôs *paws-itively* hooked!\"**  \n",
      "**Plot twist:** *Turns out the book was *dog*-eared, and now he‚Äôs floating in existential dread.* üìöüêïüåÄ  \n",
      "\n",
      "---  \n",
      "\n",
      "Now it‚Äôs a *purr-fect* combo of puns *and* chaotic surprises! üò∏üé≤üî•\n"
     ]
    }
   ],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from IPython.display import Image, display\n",
    "\n",
    "\n",
    "# Graph state\n",
    "class State(TypedDict):\n",
    "    topic: str\n",
    "    joke: str\n",
    "    improved_joke: str\n",
    "    final_joke: str\n",
    "\n",
    "\n",
    "# Nodes\n",
    "def generate_joke(state: State):\n",
    "    \"\"\"First LLM call to generate initial joke\"\"\"\n",
    "\n",
    "    msg = llm.invoke(f\"Write a short joke about {state['topic']}\")\n",
    "    return {\"joke\": msg.content}\n",
    "\n",
    "\n",
    "def check_punchline(state: State):\n",
    "    \"\"\"Gate function to check if the joke has a punchline\"\"\"\n",
    "\n",
    "    # Simple check - does the joke contain \"?\" or \"!\"\n",
    "    if \"?\" in state[\"joke\"] or \"!\" in state[\"joke\"]:\n",
    "        return \"Fail\"\n",
    "    return \"Pass\"\n",
    "\n",
    "\n",
    "def improve_joke(state: State):\n",
    "    \"\"\"Second LLM call to improve the joke\"\"\"\n",
    "\n",
    "    msg = llm.invoke(f\"Make this joke funnier by adding wordplay: {state['joke']}\")\n",
    "    return {\"improved_joke\": msg.content}\n",
    "\n",
    "\n",
    "def polish_joke(state: State):\n",
    "    \"\"\"Third LLM call for final polish\"\"\"\n",
    "\n",
    "    msg = llm.invoke(f\"Add a surprising twist to this joke: {state['improved_joke']}\")\n",
    "    return {\"final_joke\": msg.content}\n",
    "\n",
    "\n",
    "# Build workflow\n",
    "workflow = StateGraph(State)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"generate_joke\", generate_joke)\n",
    "workflow.add_node(\"improve_joke\", improve_joke)\n",
    "workflow.add_node(\"polish_joke\", polish_joke)\n",
    "\n",
    "# Add edges to connect nodes\n",
    "workflow.add_edge(START, \"generate_joke\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate_joke\", check_punchline, {\"Fail\": \"improve_joke\", \"Pass\": END}\n",
    ")\n",
    "workflow.add_edge(\"improve_joke\", \"polish_joke\")\n",
    "workflow.add_edge(\"polish_joke\", END)\n",
    "\n",
    "# Compile\n",
    "chain = workflow.compile()\n",
    "\n",
    "# Invoke\n",
    "state = chain.invoke({\"topic\": \"cats\"})\n",
    "print(\"Initial joke:\")\n",
    "print(state[\"joke\"])\n",
    "print(\"\\n--- --- ---\\n\")\n",
    "if \"improved_joke\" in state:\n",
    "    print(\"Improved joke:\")\n",
    "    print(state[\"improved_joke\"])\n",
    "    print(\"\\n--- --- ---\\n\")\n",
    "\n",
    "    print(\"Final joke:\")\n",
    "    print(state[\"final_joke\"])\n",
    "else:\n",
    "    print(\"Joke failed quality gate - no punchline detected!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a story, joke, and poem about cats!\n",
      "\n",
      "STORY:\n",
      "**The Secret Kingdom of Whiskerfell**  \n",
      "\n",
      "In a quiet little town, nestled between rolling hills and dense forests, there was a place known only to cats‚Äî**Whiskerfell**, a hidden kingdom where felines ruled with grace and cunning.  \n",
      "\n",
      "By day, the cats of the town were ordinary pets‚Äînapping in sunbeams, chasing yarn, and occasionally knocking things off tables. But when the moon rose high, they slipped through hidden doorways, vanishing into the shadows.  \n",
      "\n",
      "One night, a curious kitten named **Pip** followed his older sister, Luna, as she darted behind an old oak tree. To his amazement, the bark shimmered, and he tumbled into a world of towering catnip castles, glowing firefly lanterns, and cats of all shapes and sizes gathered in a grand council.  \n",
      "\n",
      "\"Who dares enter Whiskerfell uninvited?\" rumbled **Lord Tiberius**, a massive Maine Coon with a golden mane.  \n",
      "\n",
      "Luna sighed. \"This is Pip, my little brother. He followed me.\"  \n",
      "\n",
      "The council murmured. Outsiders‚Äîespecially kittens‚Äîwere rare in Whiskerfell. But before they could decide his fate, a shriek echoed through the kingdom.  \n",
      "\n",
      "\"The **Shadow Paws** are attacking!\" yowled a tabby guard.  \n",
      "\n",
      "A gang of rogue cats, led by the sleek and sinister **Onyx**, had come to steal the legendary **Jewel of Purrs**, the source of Whiskerfell‚Äôs magic.  \n",
      "\n",
      "Pip‚Äôs heart pounded, but he had an idea. \"I know where they‚Äôre going!\" he squeaked. \"They‚Äôll take the tunnel under the Milk River!\"  \n",
      "\n",
      "The council hesitated‚Äîcould they trust a kitten? But Luna nodded. \"He‚Äôs small and fast. He can help.\"  \n",
      "\n",
      "With a flick of his tail, Lord Tiberius agreed. \"Very well. Pip, lead the way!\"  \n",
      "\n",
      "Using his tiny size to slip through narrow passages, Pip guided the royal guards to ambush the Shadow Paws. In the chaos, he snatched the Jewel from Onyx‚Äôs grasp and darted back to safety.  \n",
      "\n",
      "The kingdom erupted in cheers. Lord Tiberius bowed his head. \"You have proven yourself, Pip. From this night on, you are a guardian of Whiskerfell.\"  \n",
      "\n",
      "As dawn approached, the cats returned to their human homes, their secret adventure unknown to their owners. And though Pip still napped in sunbeams by day, he now dreamed of moonlit battles, hidden kingdoms, and the next great adventure awaiting him in **Whiskerfell**.  \n",
      "\n",
      "**The End.** üêæ\n",
      "\n",
      "JOKE:\n",
      "Sure! Here's a purr-fect cat joke for you:  \n",
      "\n",
      "**Why don‚Äôt cats play poker in the jungle?**  \n",
      "*Because there are too many cheetahs!* üêÜüòπ  \n",
      "\n",
      "Let me know if you want more‚ÄîI've got a *litter* of them! üò∏\n",
      "\n",
      "POEM:\n",
      "**Whiskers and Grace**  \n",
      "\n",
      "Oh, cats so sly, with eyes so bright,  \n",
      "You prowl the halls in velvet light.  \n",
      "With tails held high and paws so neat,  \n",
      "You dance on shadows, soft and fleet.  \n",
      "\n",
      "A flicker here, a twitch there,  \n",
      "A hunter‚Äôs gaze, a midnight stare.  \n",
      "You leap so high, you land so light,  \n",
      "A ghostly blur‚Äîthen out of sight.  \n",
      "\n",
      "You curl in sunbeams, warm and deep,  \n",
      "Or stretch and yawn from lazy sleep.  \n",
      "Yet at a sound‚Äîa mouse, a bird‚Äî  \n",
      "Your ears perk up, your claws are stirred.  \n",
      "\n",
      "Oh, noble beast, both wild and sweet,  \n",
      "With purrs like storms and hearts so fleet.  \n",
      "You rule our homes with quiet might‚Äî  \n",
      "A tiny lion, draped in night.\n"
     ]
    }
   ],
   "source": [
    "# Graph state\n",
    "class State(TypedDict):\n",
    "    topic: str\n",
    "    joke: str\n",
    "    story: str\n",
    "    poem: str\n",
    "    combined_output: str\n",
    "\n",
    "\n",
    "# Nodes\n",
    "def call_llm_1(state: State):\n",
    "    \"\"\"First LLM call to generate initial joke\"\"\"\n",
    "\n",
    "    msg = llm.invoke(f\"Write a joke about {state['topic']}\")\n",
    "    return {\"joke\": msg.content}\n",
    "\n",
    "\n",
    "def call_llm_2(state: State):\n",
    "    \"\"\"Second LLM call to generate story\"\"\"\n",
    "\n",
    "    msg = llm.invoke(f\"Write a story about {state['topic']}\")\n",
    "    return {\"story\": msg.content}\n",
    "\n",
    "\n",
    "def call_llm_3(state: State):\n",
    "    \"\"\"Third LLM call to generate poem\"\"\"\n",
    "\n",
    "    msg = llm.invoke(f\"Write a poem about {state['topic']}\")\n",
    "    return {\"poem\": msg.content}\n",
    "\n",
    "\n",
    "def aggregator(state: State):\n",
    "    \"\"\"Combine the joke and story into a single output\"\"\"\n",
    "\n",
    "    combined = f\"Here's a story, joke, and poem about {state['topic']}!\\n\\n\"\n",
    "    combined += f\"STORY:\\n{state['story']}\\n\\n\"\n",
    "    combined += f\"JOKE:\\n{state['joke']}\\n\\n\"\n",
    "    combined += f\"POEM:\\n{state['poem']}\"\n",
    "    return {\"combined_output\": combined}\n",
    "\n",
    "\n",
    "# Build workflow\n",
    "parallel_builder = StateGraph(State)\n",
    "\n",
    "# Add nodes\n",
    "parallel_builder.add_node(\"call_llm_1\", call_llm_1)\n",
    "parallel_builder.add_node(\"call_llm_2\", call_llm_2)\n",
    "parallel_builder.add_node(\"call_llm_3\", call_llm_3)\n",
    "parallel_builder.add_node(\"aggregator\", aggregator)\n",
    "\n",
    "# Add edges to connect nodes\n",
    "parallel_builder.add_edge(START, \"call_llm_1\")\n",
    "parallel_builder.add_edge(START, \"call_llm_2\")\n",
    "parallel_builder.add_edge(START, \"call_llm_3\")\n",
    "parallel_builder.add_edge(\"call_llm_1\", \"aggregator\")\n",
    "parallel_builder.add_edge(\"call_llm_2\", \"aggregator\")\n",
    "parallel_builder.add_edge(\"call_llm_3\", \"aggregator\")\n",
    "parallel_builder.add_edge(\"aggregator\", END)\n",
    "parallel_workflow = parallel_builder.compile()\n",
    "\n",
    "\n",
    "# Invoke\n",
    "state = parallel_workflow.invoke({\"topic\": \"cats\"})\n",
    "print(state[\"combined_output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here's a purr-fect cat joke for you:  \n",
      "\n",
      "**Why don‚Äôt cats play poker in the jungle?**  \n",
      "*Because there are too many cheetahs!* üêÜüòπ  \n",
      "\n",
      "Let me know if you want more‚ÄîI‚Äôve got a *litter* of them! üò∏\n"
     ]
    }
   ],
   "source": [
    "from typing_extensions import Literal\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "\n",
    "# Schema for structured output to use as routing logic\n",
    "class Route(BaseModel):\n",
    "    step: Literal[\"poem\", \"story\", \"joke\"] = Field(\n",
    "        None, description=\"The next step in the routing process\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Augment the LLM with schema for structured output\n",
    "router = llm.with_structured_output(Route)\n",
    "\n",
    "\n",
    "# State\n",
    "class State(TypedDict):\n",
    "    input: str\n",
    "    decision: str\n",
    "    output: str\n",
    "\n",
    "\n",
    "# Nodes\n",
    "def llm_call_1(state: State):\n",
    "    \"\"\"Write a story\"\"\"\n",
    "\n",
    "    result = llm.invoke(state[\"input\"])\n",
    "    return {\"output\": result.content}\n",
    "\n",
    "\n",
    "def llm_call_2(state: State):\n",
    "    \"\"\"Write a joke\"\"\"\n",
    "\n",
    "    result = llm.invoke(state[\"input\"])\n",
    "    return {\"output\": result.content}\n",
    "\n",
    "\n",
    "def llm_call_3(state: State):\n",
    "    \"\"\"Write a poem\"\"\"\n",
    "\n",
    "    result = llm.invoke(state[\"input\"])\n",
    "    return {\"output\": result.content}\n",
    "\n",
    "\n",
    "def llm_call_router(state: State):\n",
    "    \"\"\"Route the input to the appropriate node\"\"\"\n",
    "\n",
    "    # Run the augmented LLM with structured output to serve as routing logic\n",
    "    decision = router.invoke(\n",
    "        [\n",
    "            SystemMessage(\n",
    "                content=\"Route the input to story, joke, or poem based on the user's request.\"\n",
    "            ),\n",
    "            HumanMessage(content=state[\"input\"]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return {\"decision\": decision.step}\n",
    "\n",
    "\n",
    "# Conditional edge function to route to the appropriate node\n",
    "def route_decision(state: State):\n",
    "    # Return the node name you want to visit next\n",
    "    if state[\"decision\"] == \"story\":\n",
    "        return \"llm_call_1\"\n",
    "    elif state[\"decision\"] == \"joke\":\n",
    "        return \"llm_call_2\"\n",
    "    elif state[\"decision\"] == \"poem\":\n",
    "        return \"llm_call_3\"\n",
    "\n",
    "\n",
    "# Build workflow\n",
    "router_builder = StateGraph(State)\n",
    "\n",
    "# Add nodes\n",
    "router_builder.add_node(\"llm_call_1\", llm_call_1)\n",
    "router_builder.add_node(\"llm_call_2\", llm_call_2)\n",
    "router_builder.add_node(\"llm_call_3\", llm_call_3)\n",
    "router_builder.add_node(\"llm_call_router\", llm_call_router)\n",
    "\n",
    "# Add edges to connect nodes\n",
    "router_builder.add_edge(START, \"llm_call_router\")\n",
    "router_builder.add_conditional_edges(\n",
    "    \"llm_call_router\",\n",
    "    route_decision,\n",
    "    {  # Name returned by route_decision : Name of next node to visit\n",
    "        \"llm_call_1\": \"llm_call_1\",\n",
    "        \"llm_call_2\": \"llm_call_2\",\n",
    "        \"llm_call_3\": \"llm_call_3\",\n",
    "    },\n",
    ")\n",
    "router_builder.add_edge(\"llm_call_1\", END)\n",
    "router_builder.add_edge(\"llm_call_2\", END)\n",
    "router_builder.add_edge(\"llm_call_3\", END)\n",
    "\n",
    "# Compile workflow\n",
    "router_workflow = router_builder.compile()\n",
    "\n",
    "# Invoke\n",
    "state = router_workflow.invoke({\"input\": \"Write me a joke about cats\"})\n",
    "print(state[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, List\n",
    "import operator\n",
    "\n",
    "\n",
    "# Schema for structured output to use in planning\n",
    "class Section(BaseModel):\n",
    "    name: str = Field(\n",
    "        description=\"Name for this section of the report.\",\n",
    "    )\n",
    "    description: str = Field(\n",
    "        description=\"Brief overview of the main topics and concepts to be covered in this section.\",\n",
    "    )\n",
    "\n",
    "\n",
    "class Sections(BaseModel):\n",
    "    sections: List[Section] = Field(\n",
    "        description=\"Sections of the report.\",\n",
    "    )\n",
    "\n",
    "\n",
    "# Augment the LLM with schema for structured output\n",
    "planner = llm.with_structured_output(Sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Overview of LLMs and Scaling Laws  \n",
       "\n",
       "Large Language Models (LLMs) are deep learning models trained on vast amounts of text data to perform a wide range of natural language processing (NLP) tasks, including text generation, translation, summarization, and question answering. These models, such as OpenAI's GPT series, Google's PaLM, and Meta's LLaMA, leverage transformer architectures to capture complex linguistic patterns and contextual relationships.  \n",
       "\n",
       "A key factor in the advancement of LLMs is the study of **scaling laws**, which describe how model performance improves with increases in model size, dataset size, and computational resources. Empirical research has shown that LLMs exhibit predictable improvements in capabilities when scaled along these dimensions, following power-law relationships. Understanding these scaling laws is critical for optimizing resource allocation, improving efficiency, and guiding future model development.  \n",
       "\n",
       "Scaling laws also highlight trade-offs between model size, training cost, and performance, influencing decisions in both academic research and industry applications. By leveraging these principles, researchers can design more capable and cost-effective models, pushing the boundaries of what LLMs can achieve.\n",
       "\n",
       "---\n",
       "\n",
       "# Understanding Scaling Laws  \n",
       "\n",
       "Scaling laws describe the predictable relationships between the performance of large language models (LLMs) and key variables such as model size, dataset size, and computational resources. These laws provide a mathematical framework for understanding how improvements in these factors influence model capabilities.  \n",
       "\n",
       "### Mathematical Foundations  \n",
       "Scaling laws are often expressed as power-law relationships of the form:  \n",
       "\n",
       "\\[\n",
       "L(N, D) \\approx \\left( \\frac{N_c}{N} \\right)^{\\alpha_N} + \\left( \\frac{D_c}{D} \\right)^{\\alpha_D}\n",
       "\\]  \n",
       "\n",
       "Where:  \n",
       "- \\( L \\) is the loss (a measure of model error).  \n",
       "- \\( N \\) is the number of model parameters.  \n",
       "- \\( D \\) is the dataset size.  \n",
       "- \\( N_c, D_c \\) are critical thresholds beyond which scaling becomes less effective.  \n",
       "- \\( \\alpha_N, \\alpha_D \\) are scaling exponents that determine the rate of improvement.  \n",
       "\n",
       "Empirical studies have shown that increasing \\( N \\) and \\( D \\) in tandem leads to consistent reductions in loss, with diminishing returns observed once either parameter exceeds its critical threshold.  \n",
       "\n",
       "### Empirical Observations  \n",
       "Key findings from scaling law research include:  \n",
       "1. **Compute-Optimal Scaling**: Performance improves predictably with increased compute, following a power-law trend.  \n",
       "2. **Data Efficiency**: Larger models require proportionally more data to avoid overfitting.  \n",
       "3. **Emergent Abilities**: Some capabilities appear only beyond certain scale thresholds, suggesting discontinuous improvements.  \n",
       "4. **Trade-offs**: Optimal scaling balances model size, data quantity, and training compute, often favoring larger models when resources permit.  \n",
       "\n",
       "These principles guide the development of more efficient and capable LLMs, enabling better resource allocation and performance prediction.\n",
       "\n",
       "---\n",
       "\n",
       "# Key Findings in LLM Scaling  \n",
       "\n",
       "### **1. Power-Law Scaling Between Model Size and Performance**  \n",
       "Research has consistently shown that large language model (LLM) performance follows a power-law relationship with model size (parameters), compute budget, and dataset size. Key studies (e.g., Kaplan et al., 2020) demonstrate that:  \n",
       "- **Performance scales predictably** with increasing parameters, following a smooth power-law curve.  \n",
       "- **Diminishing returns occur** beyond a certain scale, where further increases in model size yield smaller improvements.  \n",
       "\n",
       "### **2. Data Scaling is Equally Critical**  \n",
       "- **Optimal dataset size** scales with model size‚Äîlarger models require proportionally more training data to avoid underfitting.  \n",
       "- **Data quality and diversity** significantly impact performance, with some studies suggesting that filtering low-quality data improves efficiency.  \n",
       "\n",
       "### **3. Compute-Optimal Training**  \n",
       "- The **Chinchilla scaling laws** (Hoffmann et al., 2022) challenge earlier assumptions, showing that smaller models trained on more data can outperform larger models with less data, given the same compute budget.  \n",
       "- **Optimal training regimes** balance model size, dataset size, and compute, rather than simply maximizing parameters.  \n",
       "\n",
       "### **4. Emergent Abilities at Scale**  \n",
       "- Certain capabilities (e.g., reasoning, few-shot learning) **emerge only beyond a critical scale**, suggesting discontinuous improvements in performance.  \n",
       "- Larger models exhibit **better sample efficiency**, requiring fewer examples for in-context learning.  \n",
       "\n",
       "### **5. Efficiency Improvements Through Scaling**  \n",
       "- **Sparse models and mixture-of-experts (MoE)** architectures demonstrate that not all parameters need to be active during inference, improving efficiency without sacrificing performance.  \n",
       "- **Scaling laws generalize across architectures**, though variations exist based on model design (e.g., transformers vs. recurrent networks).  \n",
       "\n",
       "### **6. Challenges in Extreme Scaling**  \n",
       "- **Training instability** increases with model size, requiring techniques like better initialization and adaptive optimization.  \n",
       "- **Energy and cost constraints** make extreme scaling economically and environmentally challenging.  \n",
       "\n",
       "These findings highlight the importance of balancing model size, data, and compute to optimize performance while managing practical constraints.\n",
       "\n",
       "---\n",
       "\n",
       "# Implications of Scaling Laws  \n",
       "\n",
       "Scaling laws have profound implications for the design, training, and deployment of large language models (LLMs), influencing cost, efficiency, and performance trade-offs.  \n",
       "\n",
       "### **Design Considerations**  \n",
       "Scaling laws suggest that model performance improves predictably with increases in compute, dataset size, and model parameters. This encourages the development of larger architectures but also necessitates careful resource allocation. Designers must balance:  \n",
       "- **Parameter Efficiency**: Larger models require more memory and compute, driving innovations in sparse architectures or mixture-of-experts (MoE) models.  \n",
       "- **Hardware Constraints**: Scaling demands specialized infrastructure (e.g., GPUs/TPUs), influencing architectural choices like parallelism strategies (data, model, or pipeline parallelism).  \n",
       "\n",
       "### **Training Costs and Efficiency**  \n",
       "Training LLMs at scale incurs exponential costs in compute and energy. Key trade-offs include:  \n",
       "- **Diminishing Returns**: Performance gains slow as models grow, requiring optimization techniques like curriculum learning or better data filtering.  \n",
       "- **Budget Constraints**: Organizations must weigh the benefits of larger models against prohibitive training expenses, leading to increased interest in efficient pretraining (e.g., Chinchilla‚Äôs compute-optimal scaling).  \n",
       "\n",
       "### **Deployment Challenges**  \n",
       "Scaling laws also affect real-world usability:  \n",
       "- **Inference Costs**: Larger models demand more resources for inference, prompting techniques like quantization, distillation, or on-demand scaling.  \n",
       "- **Latency vs. Accuracy**: High-parameter models may offer better accuracy but suffer from slower response times, necessitating trade-offs for latency-sensitive applications.  \n",
       "\n",
       "### **Future Directions**  \n",
       "As scaling continues, research focuses on:  \n",
       "- **Sustainable Scaling**: Reducing energy consumption via efficient architectures.  \n",
       "- **Alternative Paradigms**: Exploring smaller, specialized models or retrieval-augmented approaches to mitigate scaling costs.  \n",
       "\n",
       "Ultimately, scaling laws guide‚Äîbut also constrain‚ÄîLLM development, requiring a strategic balance between performance gains and practical feasibility.\n",
       "\n",
       "---\n",
       "\n",
       "# Challenges and Limitations  \n",
       "\n",
       "Scaling laws, while powerful for improving model performance, come with several challenges and limitations that must be carefully considered.  \n",
       "\n",
       "### Diminishing Returns  \n",
       "As models grow larger, the marginal gains in performance often decrease. Early scaling may yield significant improvements, but beyond a certain point, increasing parameters or data leads to only incremental benefits. This phenomenon raises questions about the cost-effectiveness of further scaling, especially given the substantial resources required.  \n",
       "\n",
       "### Computational Constraints  \n",
       "Training large-scale models demands immense computational power, energy, and infrastructure. The costs associated with hardware, electricity, and cooling can be prohibitive for many organizations. Additionally, longer training times and the need for specialized hardware (e.g., GPUs, TPUs) create barriers to entry for smaller research groups and institutions.  \n",
       "\n",
       "### Ethical Considerations  \n",
       "The environmental impact of training large models is a growing concern, as energy consumption contributes to carbon emissions. Furthermore, scaling can amplify biases present in training data, leading to unfair or harmful outputs. The concentration of AI development in a few well-resourced entities also raises concerns about accessibility and equitable distribution of benefits.  \n",
       "\n",
       "### Practical Deployment Challenges  \n",
       "Even if a model performs well in controlled settings, deploying it in real-world applications may introduce latency, memory constraints, and maintenance difficulties. Smaller, more efficient models are often preferred for production environments, highlighting a trade-off between scale and usability.  \n",
       "\n",
       "Addressing these challenges requires balancing performance gains with sustainability, fairness, and practical feasibility. Future research must explore techniques like efficient architectures, data pruning, and ethical AI practices to mitigate these limitations.\n",
       "\n",
       "---\n",
       "\n",
       "# Future Directions  \n",
       "\n",
       "Potential future research directions and innovations in LLM scaling encompass several emerging trends and alternative approaches aimed at improving efficiency, performance, and accessibility.  \n",
       "\n",
       "### **Alternative Scaling Approaches**  \n",
       "1. **Sparse Models & Mixture-of-Experts (MoE):**  \n",
       "   - Investigating dynamic routing mechanisms to activate only relevant model components, reducing computational costs.  \n",
       "   - Optimizing expert selection and load balancing for improved inference efficiency.  \n",
       "\n",
       "2. **Efficient Training Paradigms:**  \n",
       "   - Exploring curriculum learning and progressive training strategies to enhance model convergence.  \n",
       "   - Investigating data pruning and importance sampling to reduce redundant training data.  \n",
       "\n",
       "3. **Hardware-Aware Optimization:**  \n",
       "   - Developing specialized architectures (e.g., neuromorphic computing) for LLM inference.  \n",
       "   - Leveraging quantization, distillation, and sparsity to enable deployment on edge devices.  \n",
       "\n",
       "### **Emerging Trends**  \n",
       "1. **Multimodal Scaling:**  \n",
       "   - Extending LLMs to integrate vision, audio, and other modalities for richer contextual understanding.  \n",
       "   - Investigating cross-modal attention mechanisms for seamless multimodal reasoning.  \n",
       "\n",
       "2. **Decentralized & Federated Learning:**  \n",
       "   - Enabling collaborative training across distributed datasets while preserving privacy.  \n",
       "   - Exploring blockchain-based incentives for decentralized model contributions.  \n",
       "\n",
       "3. **Self-Improving Models:**  \n",
       "   - Developing LLMs capable of iterative self-reflection and refinement via reinforcement learning.  \n",
       "   - Investigating automated hyperparameter tuning and architecture search.  \n",
       "\n",
       "4. **Ethical & Sustainable Scaling:**  \n",
       "   - Reducing carbon footprints via energy-efficient training methods.  \n",
       "   - Implementing fairness-aware scaling to mitigate biases in large models.  \n",
       "\n",
       "These directions aim to push the boundaries of LLM capabilities while addressing computational, ethical, and practical challenges.\n",
       "\n",
       "---\n",
       "\n",
       "# Conclusion  \n",
       "\n",
       "Scaling laws have emerged as a foundational principle in the development of artificial intelligence, particularly for large language models (LLMs). The empirical relationship between model size, dataset size, compute resources, and performance has provided a roadmap for optimizing AI systems. Key takeaways include:  \n",
       "\n",
       "1. **Predictable Performance Gains** ‚Äì Scaling laws demonstrate that increasing model parameters, training data, or compute yields consistent improvements in performance, enabling more accurate forecasting of resource requirements.  \n",
       "2. **Diminishing Returns** ‚Äì While scaling remains effective, the marginal gains decrease as models grow, necessitating more efficient architectures and training techniques.  \n",
       "3. **Broader Implications for AI Research** ‚Äì These laws influence not only LLMs but also multimodal and domain-specific models, reinforcing the importance of systematic scaling in AI advancement.  \n",
       "4. **Economic and Ethical Considerations** ‚Äì The resource-intensive nature of scaling raises concerns about accessibility, environmental impact, and the concentration of AI development in well-funded organizations.  \n",
       "\n",
       "Moving forward, the field must balance continued scaling with innovations in efficiency, interpretability, and equitable access to ensure sustainable progress in AI. Scaling laws will remain a critical tool, but their limitations highlight the need for complementary breakthroughs in model design and training methodologies."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langgraph.constants import Send\n",
    "\n",
    "\n",
    "# Graph state\n",
    "class State(TypedDict):\n",
    "    topic: str  # Report topic\n",
    "    sections: list[Section]  # List of report sections\n",
    "    completed_sections: Annotated[\n",
    "        list, operator.add\n",
    "    ]  # All workers write to this key in parallel\n",
    "    final_report: str  # Final report\n",
    "\n",
    "\n",
    "# Worker state\n",
    "class WorkerState(TypedDict):\n",
    "    section: Section\n",
    "    completed_sections: Annotated[list, operator.add]\n",
    "\n",
    "\n",
    "# Nodes\n",
    "def orchestrator(state: State):\n",
    "    \"\"\"Orchestrator that generates a plan for the report\"\"\"\n",
    "\n",
    "    # Generate queries\n",
    "    report_sections = planner.invoke(\n",
    "        [\n",
    "            SystemMessage(content=\"Generate a plan for the report.\"),\n",
    "            HumanMessage(content=f\"Here is the report topic: {state['topic']}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return {\"sections\": report_sections.sections}\n",
    "\n",
    "\n",
    "def llm_call(state: WorkerState):\n",
    "    \"\"\"Worker writes a section of the report\"\"\"\n",
    "\n",
    "    # Generate section\n",
    "    section = llm.invoke(\n",
    "        [\n",
    "            SystemMessage(\n",
    "                content=\"Write a report section following the provided name and description. Include no preamble for each section. Use markdown formatting.\"\n",
    "            ),\n",
    "            HumanMessage(\n",
    "                content=f\"Here is the section name: {state['section'].name} and description: {state['section'].description}\"\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Write the updated section to completed sections\n",
    "    return {\"completed_sections\": [section.content]}\n",
    "\n",
    "\n",
    "def synthesizer(state: State):\n",
    "    \"\"\"Synthesize full report from sections\"\"\"\n",
    "\n",
    "    # List of completed sections\n",
    "    completed_sections = state[\"completed_sections\"]\n",
    "\n",
    "    # Format completed section to str to use as context for final sections\n",
    "    completed_report_sections = \"\\n\\n---\\n\\n\".join(completed_sections)\n",
    "\n",
    "    return {\"final_report\": completed_report_sections}\n",
    "\n",
    "\n",
    "# Conditional edge function to create llm_call workers that each write a section of the report\n",
    "def assign_workers(state: State):\n",
    "    \"\"\"Assign a worker to each section in the plan\"\"\"\n",
    "\n",
    "    # Kick off section writing in parallel via Send() API\n",
    "    return [Send(\"llm_call\", {\"section\": s}) for s in state[\"sections\"]]\n",
    "\n",
    "\n",
    "# Build workflow\n",
    "orchestrator_worker_builder = StateGraph(State)\n",
    "\n",
    "# Add the nodes\n",
    "orchestrator_worker_builder.add_node(\"orchestrator\", orchestrator)\n",
    "orchestrator_worker_builder.add_node(\"llm_call\", llm_call)\n",
    "orchestrator_worker_builder.add_node(\"synthesizer\", synthesizer)\n",
    "\n",
    "# Add edges to connect nodes\n",
    "orchestrator_worker_builder.add_edge(START, \"orchestrator\")\n",
    "orchestrator_worker_builder.add_conditional_edges(\n",
    "    \"orchestrator\", assign_workers, [\"llm_call\"]\n",
    ")\n",
    "orchestrator_worker_builder.add_edge(\"llm_call\", \"synthesizer\")\n",
    "orchestrator_worker_builder.add_edge(\"synthesizer\", END)\n",
    "\n",
    "# Compile the workflow\n",
    "orchestrator_worker = orchestrator_worker_builder.compile()\n",
    "\n",
    "\n",
    "# Invoke\n",
    "state = orchestrator_worker.invoke({\"topic\": \"Create a report on LLM scaling laws\"})\n",
    "\n",
    "from IPython.display import Markdown\n",
    "Markdown(state[\"final_report\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here's a purr-fect cat joke for you:  \n",
      "\n",
      "**Why don‚Äôt cats play poker in the wild?**  \n",
      "*Because there are too many cheetahs!* üêÜüòπ  \n",
      "\n",
      "(Or if you prefer a classic: **\"Why was the cat sitting on the computer?** *To keep an eye on the mouse!*\" üñ±Ô∏èüê±)  \n",
      "\n",
      "Let me know if you want more‚ÄîI've got a *litter* of them! üò∏\n"
     ]
    }
   ],
   "source": [
    "# Graph state\n",
    "class State(TypedDict):\n",
    "    joke: str\n",
    "    topic: str\n",
    "    feedback: str\n",
    "    funny_or_not: str\n",
    "\n",
    "\n",
    "# Schema for structured output to use in evaluation\n",
    "class Feedback(BaseModel):\n",
    "    grade: Literal[\"funny\", \"not funny\"] = Field(\n",
    "        description=\"Decide if the joke is funny or not.\",\n",
    "    )\n",
    "    feedback: str = Field(\n",
    "        description=\"If the joke is not funny, provide feedback on how to improve it.\",\n",
    "    )\n",
    "\n",
    "\n",
    "# Augment the LLM with schema for structured output\n",
    "evaluator = llm.with_structured_output(Feedback)\n",
    "\n",
    "\n",
    "# Nodes\n",
    "def llm_call_generator(state: State):\n",
    "    \"\"\"LLM generates a joke\"\"\"\n",
    "\n",
    "    if state.get(\"feedback\"):\n",
    "        msg = llm.invoke(\n",
    "            f\"Write a joke about {state['topic']} but take into account the feedback: {state['feedback']}\"\n",
    "        )\n",
    "    else:\n",
    "        msg = llm.invoke(f\"Write a joke about {state['topic']}\")\n",
    "    return {\"joke\": msg.content}\n",
    "\n",
    "\n",
    "def llm_call_evaluator(state: State):\n",
    "    \"\"\"LLM evaluates the joke\"\"\"\n",
    "\n",
    "    grade = evaluator.invoke(f\"Grade the joke {state['joke']}\")\n",
    "    return {\"funny_or_not\": grade.grade, \"feedback\": grade.feedback}\n",
    "\n",
    "\n",
    "# Conditional edge function to route back to joke generator or end based upon feedback from the evaluator\n",
    "def route_joke(state: State):\n",
    "    \"\"\"Route back to joke generator or end based upon feedback from the evaluator\"\"\"\n",
    "\n",
    "    if state[\"funny_or_not\"] == \"funny\":\n",
    "        return \"Accepted\"\n",
    "    elif state[\"funny_or_not\"] == \"not funny\":\n",
    "        return \"Rejected + Feedback\"\n",
    "\n",
    "\n",
    "# Build workflow\n",
    "optimizer_builder = StateGraph(State)\n",
    "\n",
    "# Add the nodes\n",
    "optimizer_builder.add_node(\"llm_call_generator\", llm_call_generator)\n",
    "optimizer_builder.add_node(\"llm_call_evaluator\", llm_call_evaluator)\n",
    "\n",
    "# Add edges to connect nodes\n",
    "optimizer_builder.add_edge(START, \"llm_call_generator\")\n",
    "optimizer_builder.add_edge(\"llm_call_generator\", \"llm_call_evaluator\")\n",
    "optimizer_builder.add_conditional_edges(\n",
    "    \"llm_call_evaluator\",\n",
    "    route_joke,\n",
    "    {  # Name returned by route_joke : Name of next node to visit\n",
    "        \"Accepted\": END,\n",
    "        \"Rejected + Feedback\": \"llm_call_generator\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# Compile the workflow\n",
    "optimizer_workflow = optimizer_builder.compile()\n",
    "\n",
    "# Invoke\n",
    "state = optimizer_workflow.invoke({\"topic\": \"Cats\"})\n",
    "print(state[\"joke\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "# Define tools\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "@tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Adds a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "@tool\n",
    "def divide(a: int, b: int) -> float:\n",
    "    \"\"\"Divide a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a / b\n",
    "\n",
    "\n",
    "# Augment the LLM with tools\n",
    "tools = [add, multiply, divide]\n",
    "tools_by_name = {tool.name: tool for tool in tools}\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Add 3 and 4.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  add (call_0_ca232637-dae6-4778-866d-9f436baacc30)\n",
      " Call ID: call_0_ca232637-dae6-4778-866d-9f436baacc30\n",
      "  Args:\n",
      "    a: 3\n",
      "    b: 4\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "\n",
      "7\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The sum of 3 and 4 is 7.\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import MessagesState\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, ToolMessage\n",
    "\n",
    "\n",
    "# Nodes\n",
    "def llm_call(state: MessagesState):\n",
    "    \"\"\"LLM decides whether to call a tool or not\"\"\"\n",
    "\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            llm_with_tools.invoke(\n",
    "                [\n",
    "                    SystemMessage(\n",
    "                        content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\n",
    "                    )\n",
    "                ]\n",
    "                + state[\"messages\"]\n",
    "            )\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "def tool_node(state: dict):\n",
    "    \"\"\"Performs the tool call\"\"\"\n",
    "\n",
    "    result = []\n",
    "    for tool_call in state[\"messages\"][-1].tool_calls:\n",
    "        tool = tools_by_name[tool_call[\"name\"]]\n",
    "        observation = tool.invoke(tool_call[\"args\"])\n",
    "        result.append(ToolMessage(content=observation, tool_call_id=tool_call[\"id\"]))\n",
    "    return {\"messages\": result}\n",
    "\n",
    "\n",
    "# Conditional edge function to route to the tool node or end based upon whether the LLM made a tool call\n",
    "def should_continue(state: MessagesState) -> Literal[\"environment\", END]:\n",
    "    \"\"\"Decide if we should continue the loop or stop based upon whether the LLM made a tool call\"\"\"\n",
    "\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    # If the LLM makes a tool call, then perform an action\n",
    "    if last_message.tool_calls:\n",
    "        return \"Action\"\n",
    "    # Otherwise, we stop (reply to the user)\n",
    "    return END\n",
    "\n",
    "\n",
    "# Build workflow\n",
    "agent_builder = StateGraph(MessagesState)\n",
    "\n",
    "# Add nodes\n",
    "agent_builder.add_node(\"llm_call\", llm_call)\n",
    "agent_builder.add_node(\"environment\", tool_node)\n",
    "\n",
    "# Add edges to connect nodes\n",
    "agent_builder.add_edge(START, \"llm_call\")\n",
    "agent_builder.add_conditional_edges(\n",
    "    \"llm_call\",\n",
    "    should_continue,\n",
    "    {\n",
    "        # Name returned by should_continue : Name of next node to visit\n",
    "        \"Action\": \"environment\",\n",
    "        END: END,\n",
    "    },\n",
    ")\n",
    "agent_builder.add_edge(\"environment\", \"llm_call\")\n",
    "\n",
    "# Compile the agent\n",
    "agent = agent_builder.compile()\n",
    "\n",
    "\n",
    "# Invoke\n",
    "messages = [HumanMessage(content=\"Add 3 and 4.\")]\n",
    "messages = agent.invoke({\"messages\": messages})\n",
    "for m in messages[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Add 3 and 4.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  add (call_0_9a3550fe-d693-4400-afdd-7589f5014d96)\n",
      " Call ID: call_0_9a3550fe-d693-4400-afdd-7589f5014d96\n",
      "  Args:\n",
      "    a: 3\n",
      "    b: 4\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: add\n",
      "\n",
      "7\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The sum of 3 and 4 is 7.\n"
     ]
    }
   ],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "# Pass in:\n",
    "# (1) the augmented LLM with tools\n",
    "# (2) the tools list (which is used to create the tool node)\n",
    "pre_built_agent = create_react_agent(llm, tools=tools)\n",
    "\n",
    "# Invoke\n",
    "messages = [HumanMessage(content=\"Add 3 and 4.\")]\n",
    "messages = pre_built_agent.invoke({\"messages\": messages})\n",
    "for m in messages[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import sqlite3\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "db_url = \"https://storage.googleapis.com/benchmarks-artifacts/travel-db/travel2.sqlite\"\n",
    "local_file = \"travel2.sqlite\"\n",
    "# The backup lets us restart for each tutorial section\n",
    "backup_file = \"travel2.backup.sqlite\"\n",
    "overwrite = False\n",
    "if overwrite or not os.path.exists(local_file):\n",
    "    response = requests.get(db_url)\n",
    "    response.raise_for_status()  # Ensure the request was successful\n",
    "    with open(local_file, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    # Backup - we will use this to \"reset\" our DB in each section\n",
    "    shutil.copy(local_file, backup_file)\n",
    "\n",
    "\n",
    "# Convert the flights to present time for our tutorial\n",
    "def update_dates(file):\n",
    "    shutil.copy(backup_file, file)\n",
    "    conn = sqlite3.connect(file)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    tables = pd.read_sql(\n",
    "        \"SELECT name FROM sqlite_master WHERE type='table';\", conn\n",
    "    ).name.tolist()\n",
    "    tdf = {}\n",
    "    for t in tables:\n",
    "        tdf[t] = pd.read_sql(f\"SELECT * from {t}\", conn)\n",
    "\n",
    "    example_time = pd.to_datetime(\n",
    "        tdf[\"flights\"][\"actual_departure\"].replace(\"\\\\N\", pd.NaT)\n",
    "    ).max()\n",
    "    current_time = pd.to_datetime(\"now\").tz_localize(example_time.tz)\n",
    "    time_diff = current_time - example_time\n",
    "\n",
    "    tdf[\"bookings\"][\"book_date\"] = (\n",
    "        pd.to_datetime(tdf[\"bookings\"][\"book_date\"].replace(\"\\\\N\", pd.NaT), utc=True)\n",
    "        + time_diff\n",
    "    )\n",
    "\n",
    "    datetime_columns = [\n",
    "        \"scheduled_departure\",\n",
    "        \"scheduled_arrival\",\n",
    "        \"actual_departure\",\n",
    "        \"actual_arrival\",\n",
    "    ]\n",
    "    for column in datetime_columns:\n",
    "        tdf[\"flights\"][column] = (\n",
    "            pd.to_datetime(tdf[\"flights\"][column].replace(\"\\\\N\", pd.NaT)) + time_diff\n",
    "        )\n",
    "\n",
    "    for table_name, df in tdf.items():\n",
    "        df.to_sql(table_name, conn, if_exists=\"replace\", index=False)\n",
    "    del df\n",
    "    del tdf\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "    return file\n",
    "\n",
    "\n",
    "db = update_dates(local_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
